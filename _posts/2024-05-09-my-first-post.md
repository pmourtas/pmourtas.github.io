---
layout: post
title: Deciphering Big Data - Module
subtitle: My Reflection
categories: E-Portofolio
---

## Reflection
<p></p>
<p></p>
<p><strong> Content </strong><br></p>
<p>In units 1 and 2, we had an introduction to big data technologies and data management, as well as several existing data types and formats. We evaluate the volume, variety, velocity, and veracity of data and how they have grown and developed in the last few years. We examine technologies like data mining, exploration, analysis, and presentation. Moreover, we review methodologies, tools, and techniques like cleansing, standardization, and normalization, along with some strategies and services that can make us able to handle the big data easier, extract all the required information available faster, and achieve greater accuracy (Open Sistemas, 2021).</p>

Other than that, we began with our core book about data wrangling with Python (Sarkar, T. & Roychowdhury, S., 2019). From the very beginning, we got an idea about the process of data wrangling. It provided us with a brief overview of Python, showing us about lists, sets, strings, tuples, and dictionaries with a lot of solved examples and mechanisms for how to properly exploit them in different scenarios. Moreover, we explore the nature of data structures and how to handle the data filling. We examine different user-defined methods, iterators, queues, stacks, and other usable tools in Python. In addition, we familiarize ourselves with some library packages, such as the lambda expression and file-handling operations for opening and closing files, as well as reading and writing to them. Lastly, we proceed further with the libraries of NumPy, Pandas, and Matplotlib, as well as their significant features, facilitating different methods each time.

In the second book, we learned about importing data, along with some tricks and techniques, from specific format files like CSV, JSON, XML, and Excel, as well as some machine command lines, in order to navigate through our folders and files on our computers (Kazil, J & Jarmul, K., 2016). That book helped us to comprehend the idea of parsing the data and how we wanted the data to be shown on our screen. It also helped us to find what we were looking for and what else we could actually do to get closer to a suitable solution, examining the numerous ways that Python and its libraries provided.

Lastly, we had a collaborative discussion about the internet of things (IoT) (Huxley et al., 2020). In that discussion, we had to highlight the opportunities, limitations, risks, and challenges in relation to data collection and determine appropriate measures to mitigate them, using appropriate academic literature. The goal of this discussion is to interact with our peers, exchanging opinions and ideas, while our tutor, through his feedback, helps us develop our research skills even more. You can see the discussion in Appendix (a).

In units 3, 4, and 5, we examine concepts about data collection and storage, data cleaning and transformation, and automating data collection. We experienced various significant sources from which we could extract useful data and investigate whether that data was reliable and verified. Other than that, we examine the different types of database storage, web services, and cloud-based platforms. We also review the use of APIs and how to identify useful content from HTML sites. An excellent tool for mining data.

Moreover, we review the data cleaning and data management pipeline, which consists of the stages of capturing raw data, cleaning the data, integrating it, designing a database, performing data analysis, and visualizing the data for presentation. In addition, it also pointed out to us the first steps to automate the whole procedure into smaller parts so as to become more time-efficient.

From our core book, we studied further Python. We learned about filtering, subsetting, and grouping with different approaches and methods. We examined methods to detect outliers and handle missing values in our data. Moreover, we learned how to concatenate, merge, and join tables or datasets. Other than that, we investigate web parsing methods with several solved examples. Lastly, we got an idea of how to handle messy data, generate expressions, and work with the modulus operator and matching patterns in order to detect and transform any inconsistencies.

In the second book, we repeated some of our studies on data cleaning methods, data formatting, and storage platforms so as to strengthen our overall knowledge with different examples and case studies. However, we get a step ahead in terms of automation methods and how to control them in terms of monitoring them and setting alarms when a problem arises.

Lastly, we had a web scraping activity in order to apply our knowledge. We had to write a script in Python to parse a string from a web page of our liking and store the result in an XML or JSON file. Our result is in Appendix (b).

In Unit 6, we took a look at database designs and normalizations. We familiarized ourselves with primary and foreign keys in relational databases, along with their several normal forms. We also learned how those forms help our datasets be less vulnerable to several anomalies and inconsistencies. Normal forms seemed to be crucial.

Lastly, in that unit, we had an assessment, a team project, where we had to team up with our peers. In that project, we had to design and present a database that satisfied all of our fictional client business requirements with all the prerequisite information about the design structure, the building process, and the evaluation of implementing the cleaning techniques for the data we had to capture from several sources.

In Unit 7, we got involved with the construction of normalized tables and database buildings. We read about relationship forms and how to implement them with the use of SQL commands (Letkowski, J., 2015). We also examine how the databases are categorized according to their data structures (IBM, 2010) and how they are compared among each other (Taipalus, T., 2024). However, the last material was the greatest as it analyzed all the aspects of database types, their concepts, where they can be implemented, and why (Chen, W., 2023). It mentioned Microsoft Access, SQL, NoSQL, and T-SQL and how they differ. It also explained to us how to apply a few SQL commands, ER diagrams, normalizations, and why the latest is so important (Database Star, 2024).

Lastly, in that unit, we also had an assessment. We had to normalize an unnormalized dataset to its first three normal forms. In addition, we had to build its third final form in a relational database system, demonstrating our knowledge of primary and foreign keys. Our result is in Appendix (c).

In Unit 8, we review compliance, security, and regulations regarding data management. We learn how to deal with those and their challenges as well. We evolved with standard protocols, GDPR articles, and the most important compliance goals, such as access control, encryption, and how to harden the overall security of applications and services in a business.

In that unit, we had another collaborative discussion among our peers and tutor. This time the topic was about the differences that may exist in GDPR rules regarding personal data compared to the ICO in the UK (ICO, 2024). It turned out that there are a few differences. You can see the discussion in Appendix (d).

In Unit 9, we took a closer look at models of database management systems. We examined the ways we can control the overall access to a database. We also review the components of databases and how each one of them matters for their creation. We learned that databases contain schemas, which describe the physical and logical paths of databases. That artifact contains crucial information about databases, and users can interact to optimize the security and access control measures (Solar Winds, 2024). 

In Unit 10, we indulged ourselves more in APIs regarding their communication process, their overall functionality, and their challenges. We familiarized ourselves with several types of APIs and how they developed over the last few years (Cooksey, B., 2014). Other than that, we learned the way a database should be built, with all the stages we have to follow before and after the build. These stages involve questioning and communication with the businesses so as to understand their needs, aspects, and goals in the market (Connolly, T. M. & Begg, C. E., 2015). We also made use of the “Dream Home” case study, which helped us better understand the whole procedure in depth.

Lastly, we had an assessment regarding the evaluation of API security requirements. Our result is in Appendix (e).

In Unit 11, we introduced the transaction processing of databases and their ACID properties, which stand for atomicity, consistency, isolation, and durability (Data Bricks, 2024). A transaction helps the storage in databases be in a consistent state, providing us with checkpoints through the recording logs with the details of all the transactions. This is crucial for the backup of a database. Other than that, we engaged in some SQL coding exercises and real-life data wrangling tasks among databases.

Moreover, we had an assessment that followed that of Unit 6. We had to clarify our suggestion of the database type for our fictional client. We should include an executive summary of all the required stages that we passed to form our final decision, such as pointing out the strengths and weaknesses of our relevant choice in relation to the other options available, explaining our regulatory compliance, and mentioning the prerequisite justification for all of those.

In our final Unit 12, we explored the future of big data analytics and the emerging new trends. We also examined the role of machine learning and its applications nowadays. Other than that, we got an introduction to artificial intelligence and its policy implications that currently restrict its use in practice (Bhatnagar, A. & Gajjar, D., 2024). It is important for scientists to keep pace with all the new emerging artifacts and technology tools that occur in the market because some of them can be beneficial for their research in terms of efficiency, cost, ease of handling, etc.

Last but not least, every module consists of live streaming seminars with our tutors, where students can also interact with questions. In those seminars, tutors explain in summary the units that we have done while adding more semantics and relevant topics as “food” for thought. For example, in our penultimate seminar, our tutor gave us more materials considering the differences in certain software for DBMS implementation, such as Hadoop, Spark, MongoDB, and MySQL, explaining also, which of them is more appropriate for different tasks (Simplilearn, 2023). That was very helpful because the students have to know exactly what type of DBMS could help them better in their domain scope. 
<p></p>
<p></p>

<p><strong> Discussion </strong><br></p>
This module was an informative tool, like all the previous modules in our program. It helps us to develop our overall knowledge of Python, Databases, APIs, and their peripherals regarding security, regulations, challenges, and limitations. Of course, any academic tool is valuable because it develops our minds, making us better human beings (University of People, 2023). Moreover, they give you the ability to grow in a professional domain, making you an invaluable tool for society and businesses around the world.

However, this module had its challenges as well. Firstly, it was not as organized as it should be. For example, the team project in unit 7 should be placed in the latest units because all the appropriate content for our project was offered in later sections right after unit 7. In addition, some of our books with Python code had not been updated for a long time. Due to that, some coding was not working as it had to. For that reason, we had to do our own research in order to solve them. So, the latest problem had its positives too.

How good this module was, depends differently on each individual. It is crucial for individuals to know how to best use it as a tool. For example, when I was reading a big code in Python, I was looking for an alternative code, smaller in size, to do the same thing or even better. Also, sometimes, when I found a difficulty, I was looking for a more comprehended code.

In my opinion, a reflection exercise like that is not so efficient and is not helping the students. Especially when that takes place in every module. It would be better to work hard on a business project using our overall knowledge of the specific module. In that way, students can be focused on the scope of their studies, which is the most important part of their program. Businesses would appreciate that even more (Warta, Ashl., 2023). 

Last but not least, I noticed that in the collaborative discussions, which are open to all, some students use Artificial Intelligence like ChatGPT in order to post their opinions (Habib, S., 2024). It is not bad to take advantage of it because it provides you with knowledge, but it is bad when you copy-paste its answer. In that way, students do not learn. Of course, the same applies to our program notes. I am talking about our core book, which has a lot of exercises whose solutions are given by AI tools. As an example:
<p></p>

![pic1](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/1.png?raw=true)
<p>Figure 1: Exercise 94, Unit 7. (Sarkar, T. & Roychowdhury, S., 2019)</p>
<p></p>

![pic2](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/2.png?raw=true)
<p>Figure 2: AI detector for the exercise 94. (Quill Bot, 2024)</p>
<p></p>


Essex University highlights in every module that it takes academic integrity very seriously. However, it seems that is not true. Giving us exercise notes with AI solutions, Essex University, as a model for its students, actually promotes the use of AI in that way, which is very sad. I hope in the future, teachers can detect and tackle the specific use of AI, especially when we are talking about copy-paste. Students should be able to use and rely on their own knowledge and skills too (Kyoungwon, S., 2021).
<p></p>
<p></p>

<p><strong> Appendix </strong><br></p>

<p>a)</p>

![pic3](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/3.png?raw=true)

![pic4](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/4.png?raw=true)

<p>Figures 3: Collaborative discussion of Unit 1.</p>
<p></p>


<p>b)</p>

![pic5](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/5.png?raw=true)

<p>Figure 4: Web scraping code for activity in Unit 3.</p>
<p></p>


<p>c)</p>

![pic6](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/6.png?raw=true)

<p>Figure 5: The table with data in unnormalized form.</p>
<p></p>

![pic7](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/7.png?raw=true)

<p>Figure 6: The first normal form of the table in figure 5.</p>                                                  
<p>Each attribute contains only atomic values and there are no repeating groups or arrays.</p>
<p></p>

![pic8](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/8.png?raw=true)

<p>Figure 7: The second normal form of the table in figure 5.</p>                                                 
<p>All attributes are fully dependent on the primary key.</p>
<p></p>

![pic9](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/9.png?raw=true)

<p>Figure 8: The third normal form of the table in figure 5.</p>                                                   
<p>Each attribute is dependent on the Primary key.</p>                                                               
<p>We also have foreign keys that show the relationships between the tables.</p>
<p></p>

![pic10](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/10.png?raw=true)

<p>Figure 9: The relational database system of the tables in figure 8.</p>
<p></p>



<p>d)</p>

![pic11](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/11.png?raw=true)

<p>Figure 10: API Security Requirements of Unit 10 activity.</p>
<p></p>


<p>e)</p>

![pic12](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/12.png?raw=true)

![pic13](https://github.com/pmourtas/pmourtas.github.io/blob/main/assets/images/banners/13.png?raw=true)

<p>Figures 11: Collaborative Discussion of Unit 8.</p>
<p></p>
<p></p>
<p></p>

<p><strong> References: </strong><br></p>
Open Sistemas (2021) MSc Data Science [Lecturecast]. Deciphering Big Data April 2024. University of Essex Online.

Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.

Kazil, J. & Jarmul, K. (2016) Data Wrangling with Python. O'Reilly. Media Inc.

Huxley et al. (2020) Data Cleaning. Sage Foundation.

Letkowski, J. (2015) Doing Database Design with MySQL. Journal of Technology Research. Available at: https://www.researchgate.net/publication/271910489_Doing_database_design_with_MySQL [Accessed 5 July 2024]

IBM (2010) What is a Database Management System? Available at: https://www.ibm.com/docs/en/zos-basic-skills?topic=zos-what-is-database-management-system [Accessed 5 July 2024]

Taipalus, T. (2024) Database management system performance comparisons: A systematic literature review. The Journal of systems and software.  208(2):111872. Available at: https://www.researchgate.net/publication/375008251_Database_management_system_performance_comparisons_A_systematic_literature_review [Accessed 5 July 2024]

Chen, W. (2023) Database Design and Implementation. ATU Faculty Open Educational Resources. Available at: https://orc.library.atu.edu/atu_oer/2/ [Accessed 5 July 2024]

Database Star (2024) Database Normalization: A Step-By-Step-Guide with Examples. Available at: https://www.databasestar.com/database-normalization/ [Accessed 5 July 2024]

ICO (2024) A guide to the data protection principles. Available at: https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-protection-principles/a-guide-to-the-data-protection-principles/ [Accessed 5 July 2024]

Solar Winds (2024) What Is a Database Schema? Available at: https://www.solarwinds.com/resources/it-glossary/database-schema [Accessed 5 July 2024]

Cooksey, B. (2014) Real-Time Communication - An Introduction to APIs. Zapier. Available at: https://zapier.com/resources/guides/apis/real-time-communication [Accessed 5 July 2024]

Connolly, T. M. & Begg, C. E. (2015) Database Systems: A Practical Approach to Design, Implementation and Management. 6th ed. Essex: Pearson.

Data Bricks (2024) What is a transaction? Available at: https://www.databricks.com/glossary/acid-transactions [Accessed 5 July 2024]

Bhatnagar, A. & Gajjar, D. (2024) Policy implications of artificial intelligence. Available at: https://post.parliament.uk/research-briefings/post-pn-0708/ [Accessed 5 July 2024]

Simplilearn (2023) What Are the Various Types of Databases? Available at: https://www.simplilearn.com/tutorials/dbms-tutorial/what-are-various-types-of-databases [Accessed 5 July 2024]

University of People (2023) Benefits of Education Are Societal and Personal. Available at: https://www.uopeople.edu/blog/benefits-of-education-are-societal-and-personal/ [Accessed 5 July 2024]

Warta, Ashl. (2023) Universities Can Do More to Prepare Students for the Workforce. Available at: https://www.jamesgmartin.center/2023/12/universities-can-do-more-to-prepare-students-for-the-workforce/ [Accessed 5 July 2024]

Habib, S. (2024) AI can help − and hurt − student creativity. Available at: https://sc.edu/uofsc/posts/2024/02/conversation-ai-help.php [Accessed 5 July 2024]

Quill Bot (2024) Free AI Detector. Available at: https://quillbot.com/ai-content-detector [Accessed 5 July 2024]

Seo, K., Tang, J., Roll, Id., Fels, S. & Yoon, D. (2021) The impact of artificial intelligence on learner–instructor interaction in online learning. International Journal of Educational Technology in Higher Education. 18,54. Available at: https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00292-9 [Accessed 5 July 2024]

